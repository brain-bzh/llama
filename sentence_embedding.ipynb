{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Sentence Embeddings\n",
    "### from  https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.32.0\n",
      "  Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /home/m20leona/.local/lib/python3.10/site-packages (from sentence-transformers) (1.4.0)\n",
      "Requirement already satisfied: Pillow in /usr/lib/python3/dist-packages (from sentence-transformers) (9.0.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/m20leona/.local/lib/python3.10/site-packages (from sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: numpy in /home/m20leona/.local/lib/python3.10/site-packages (from sentence-transformers) (1.26.2)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/m20leona/.local/lib/python3.10/site-packages (from sentence-transformers) (1.12.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/m20leona/.local/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\n",
      "Collecting huggingface-hub>=0.15.1\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 KB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/m20leona/.local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/m20leona/.local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/m20leona/.local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/m20leona/.local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: filelock in /home/m20leona/.local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: sympy in /home/m20leona/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/m20leona/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/m20leona/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/m20leona/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/m20leona/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.14.3)\n",
      "Requirement already satisfied: jinja2 in /home/m20leona/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/m20leona/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/m20leona/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/m20leona/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/m20leona/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/m20leona/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/m20leona/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/m20leona/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.0.1)\n",
      "Requirement already satisfied: networkx in /home/m20leona/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/m20leona/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->sentence-transformers) (59.6.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->sentence-transformers) (0.37.1)\n",
      "Requirement already satisfied: lit in /home/m20leona/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11.0->sentence-transformers) (17.0.6)\n",
      "Requirement already satisfied: cmake in /home/m20leona/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11.0->sentence-transformers) (3.28.1)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 KB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk->sentence-transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/m20leona/.local/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/m20leona/.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/m20leona/.local/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/m20leona/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/m20leona/.local/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Installing collected packages: sentencepiece, safetensors, regex, nltk, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed huggingface-hub-0.20.3 nltk-3.8.1 regex-2023.12.25 safetensors-0.4.2 sentence-transformers-2.3.1 sentencepiece-0.1.99 tokenizers-0.15.1 transformers-4.37.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd9a029af604f73a5516fea35aaa915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54d78c15f5c4f90bb0267a1780affc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1612ffec615a40f49a2928338025a011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5462b24e3454a6fbde303a0dbc87a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4caa2160f7564e3dbd966e91cd5bb1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385e740420d24a7fbd76ca96538fc508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/816k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864eb5884ef042cfaa518f7927b6a837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/458k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4172e96ead43f4826528cfcf374385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "#model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer_bpe = AutoTokenizer.from_pretrained(\"openai-gpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'this', 'is', 'about', 'token', '##ization', '!', '[SEP]']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= \"this is about tokenization!\"\n",
    "tokenizer_bert.tokenize(text, add_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this</w>', 'is</w>', 'about</w>', 'to', 'ken', 'ization</w>', '!</w>']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= \"this is about tokenization!\"\n",
    "tokenizer_bpe.tokenize(text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['im',\n",
       " 't</w>',\n",
       " 'atlan',\n",
       " 'ti',\n",
       " 'que</w>',\n",
       " 'is</w>',\n",
       " 'undergoing</w>',\n",
       " 'a</w>',\n",
       " 'reform</w>',\n",
       " 'of</w>',\n",
       " 'the</w>',\n",
       " 'first</w>',\n",
       " 'year</w>',\n",
       " 'teaching</w>',\n",
       " 'program</w>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= \"IMT Atlantique is undergoing a reform of the first year teaching program\"\n",
    "tokenizer_bpe.tokenize(text, add_special_tokens=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'the',\n",
       " 'im',\n",
       " '##t',\n",
       " 'at',\n",
       " '##lan',\n",
       " '##tique',\n",
       " 'engineering',\n",
       " 'school',\n",
       " 'is',\n",
       " 'undergoing',\n",
       " 'a',\n",
       " 'reform',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'year',\n",
       " 'teaching',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= \"The IMT Atlantique engineering school is undergoing a reform of the first year teaching\"\n",
    "tokenizer_bert.tokenize(text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 384)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"IMT is an engineering school\", \"First year students are many\", \"Cats are cute\"]\n",
    "embeddings = model.encode(sentences)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1887]]) IMT is an engineering school\n",
      "tensor([[0.2232]]) First year students are many\n",
      "tensor([[0.1648]]) Cats are cute\n"
     ]
    }
   ],
   "source": [
    "first_embedding = model.encode(\"The python programming course is very good\")\n",
    "for embedding, sentence in zip(embeddings, sentences):\n",
    "    similarity = util.pytorch_cos_sim(first_embedding, embedding)\n",
    "    print(similarity, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'the',\n",
       " 'im',\n",
       " '##t',\n",
       " 'director',\n",
       " 'and',\n",
       " 'the',\n",
       " 'professors',\n",
       " 'are',\n",
       " 'happy',\n",
       " 'about',\n",
       " 'the',\n",
       " 'first',\n",
       " 'year',\n",
       " 'teaching',\n",
       " 'reform',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bert=AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"The IMT director and the professors are happy about the first year teaching reform.\"\n",
    "tokenizer_bert.tokenize(text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 768])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The IMT director and the professors are happy about the first year teaching reform.\"\n",
    "\n",
    "model_bert=AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "encoded_input = tokenizer_bert(text, return_tensors=\"pt\")\n",
    "output = model_bert(**encoded_input)\n",
    "output[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cos Similarity between PROFESSORS and TEACHING 0.6519165635108948\n",
      "Cos Similarity between PROFESSORS and HAPPY  0.36290210485458374\n"
     ]
    }
   ],
   "source": [
    "prof_embedding = output[\"last_hidden_state\"][0][7]  # 7 is the position of prof\n",
    "teaching_embedding = output[\"last_hidden_state\"][0][14]  # 14 is the position of teaching\n",
    "happy_embedding= output[\"last_hidden_state\"][0][9]  # 9 is the position of happy\n",
    "\n",
    "print(f\"Cos Similarity between PROFESSORS and TEACHING {util.pytorch_cos_sim(prof_embedding, teaching_embedding)[0][0]}\")\n",
    "\n",
    "print(f\"Cos Similarity between PROFESSORS and HAPPY  {util.pytorch_cos_sim(prof_embedding, happy_embedding)[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cos Similarity between two PROFESSORS embeddings 0.4150097370147705\n"
     ]
    }
   ],
   "source": [
    "text = \"The angry and unhappy professors\"\n",
    "encoded_input = tokenizer_bert(text, return_tensors=\"pt\")\n",
    "output = model_bert(**encoded_input)\n",
    "output[\"last_hidden_state\"].shape\n",
    "prof_embedding_2 = output[\"last_hidden_state\"][0][5]\n",
    "print(f\"Cos Similarity between two PROFESSORS embeddings {util.pytorch_cos_sim(prof_embedding_2,prof_embedding)[0][0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
