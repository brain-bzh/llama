{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Sentence Embeddings\n",
    "### from  https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "#model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer_bpe = AutoTokenizer.from_pretrained(\"openai-gpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'this', 'is', 'about', 'token', '##ization', '!', '[SEP]']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= \"this is about tokenization!\"\n",
    "tokenizer_bert.tokenize(text, add_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this</w>', 'is</w>', 'about</w>', 'to', 'ken', 'ization</w>', '!</w>']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= \"this is about tokenization!\"\n",
    "tokenizer_bpe.tokenize(text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['im',\n",
       " 't</w>',\n",
       " 'atlan',\n",
       " 'ti',\n",
       " 'que</w>',\n",
       " 'is</w>',\n",
       " 'undergoing</w>',\n",
       " 'a</w>',\n",
       " 'reform</w>',\n",
       " 'of</w>',\n",
       " 'the</w>',\n",
       " 'first</w>',\n",
       " 'year</w>',\n",
       " 'teaching</w>',\n",
       " 'program</w>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= \"IMT Atlantique is undergoing a reform of the first year teaching program\"\n",
    "tokenizer_bpe.tokenize(text, add_special_tokens=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'the',\n",
       " 'im',\n",
       " '##t',\n",
       " 'at',\n",
       " '##lan',\n",
       " '##tique',\n",
       " 'engineering',\n",
       " 'school',\n",
       " 'is',\n",
       " 'undergoing',\n",
       " 'a',\n",
       " 'reform',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'year',\n",
       " 'teaching',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= \"The IMT Atlantique engineering school is undergoing a reform of the first year teaching\"\n",
    "tokenizer_bert.tokenize(text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 384)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"IMT is an engineering school\", \"First year students are many\", \"Cats are cute\"]\n",
    "embeddings = model.encode(sentences)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1887]]) IMT is an engineering school\n",
      "tensor([[0.2232]]) First year students are many\n",
      "tensor([[0.1648]]) Cats are cute\n"
     ]
    }
   ],
   "source": [
    "first_embedding = model.encode(\"The python programming course is very good\")\n",
    "for embedding, sentence in zip(embeddings, sentences):\n",
    "    similarity = util.pytorch_cos_sim(first_embedding, embedding)\n",
    "    print(similarity, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'the',\n",
       " 'im',\n",
       " '##t',\n",
       " 'director',\n",
       " 'and',\n",
       " 'the',\n",
       " 'professors',\n",
       " 'are',\n",
       " 'happy',\n",
       " 'about',\n",
       " 'the',\n",
       " 'first',\n",
       " 'year',\n",
       " 'teaching',\n",
       " 'reform',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bert=AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"The IMT director and the professors are happy about the first year teaching reform.\"\n",
    "tokenizer_bert.tokenize(text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 768])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The IMT director and the professors are happy about the first year teaching reform.\"\n",
    "\n",
    "model_bert=AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "encoded_input = tokenizer_bert(text, return_tensors=\"pt\")\n",
    "output = model_bert(**encoded_input)\n",
    "output[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cos Similarity between PROFESSORS and TEACHING 0.6519165635108948\n",
      "Cos Similarity between PROFESSORS and HAPPY  0.36290210485458374\n"
     ]
    }
   ],
   "source": [
    "prof_embedding = output[\"last_hidden_state\"][0][7]  # 7 is the position of prof\n",
    "teaching_embedding = output[\"last_hidden_state\"][0][14]  # 14 is the position of teaching\n",
    "happy_embedding= output[\"last_hidden_state\"][0][9]  # 9 is the position of happy\n",
    "\n",
    "print(f\"Cos Similarity between PROFESSORS and TEACHING {util.pytorch_cos_sim(prof_embedding, teaching_embedding)[0][0]}\")\n",
    "\n",
    "print(f\"Cos Similarity between PROFESSORS and HAPPY  {util.pytorch_cos_sim(prof_embedding, happy_embedding)[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cos Similarity between two PROFESSORS embeddings 0.4150097370147705\n"
     ]
    }
   ],
   "source": [
    "text = \"The angry and unhappy professors\"\n",
    "encoded_input = tokenizer_bert(text, return_tensors=\"pt\")\n",
    "output = model_bert(**encoded_input)\n",
    "output[\"last_hidden_state\"].shape\n",
    "prof_embedding_2 = output[\"last_hidden_state\"][0][5]\n",
    "print(f\"Cos Similarity between two PROFESSORS embeddings {util.pytorch_cos_sim(prof_embedding_2,prof_embedding)[0][0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
